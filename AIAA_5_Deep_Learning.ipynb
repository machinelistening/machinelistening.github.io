{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4218b705",
   "metadata": {},
   "source": [
    "# AI-based Audio Analysis of Music and Soundscapes\n",
    "\n",
    "## Deep Learning "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e690b683",
   "metadata": {},
   "source": [
    "## <font color='red'>Programming Session</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f5b1554",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import librosa\n",
    "import tensorflow\n",
    "import matplotlib.pyplot as pl\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "import IPython.display as ipd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b858785",
   "metadata": {},
   "source": [
    "## Classifying animal sounds with a fully-connected deep neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee71160b",
   "metadata": {},
   "source": [
    "We'll use the animal classification dataset from the lecture on **Machine Learning**\n",
    "\n",
    "**Reminder**: The dataset used here is a manual selection of 5 examples for 5 animal classes from the https://github.com/karolpiczak/ESC-50 dataset.\n",
    "\n",
    "Let's use the code from before to load the dataset..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36751d5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "dir_dataset = 'audio/animal_sounds'\n",
    "sub_directories = glob.glob(os.path.join(dir_dataset, '*'))\n",
    "\n",
    "n_sub = len(sub_directories)\n",
    "# let's collect the files in each subdirectory\n",
    "# the folder name is the class name\n",
    "fn_wav_list = []\n",
    "class_label = []\n",
    "file_num_in_class = []\n",
    "\n",
    "for i in range(n_sub):\n",
    "    current_class_label = os.path.basename(sub_directories[i])\n",
    "    current_fn_wav_list = sorted(glob.glob(os.path.join(sub_directories[i], '*.wav')))\n",
    "    for k, fn_wav in enumerate(current_fn_wav_list):\n",
    "        fn_wav_list.append(fn_wav)\n",
    "        class_label.append(current_class_label)\n",
    "        file_num_in_class.append(k)\n",
    "\n",
    "n_files = len(class_label)\n",
    "   \n",
    "# this vector includes a \"counter\" for each file within its class, we use it later ...\n",
    "file_num_in_class = np.array(file_num_in_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80d86391",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_classes = sorted(list(set(class_label)))\n",
    "print(\"All unique class labels (sorted alphabetically)\", unique_classes)\n",
    "class_id = np.array([unique_classes.index(_) for _ in class_label])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e83e076a",
   "metadata": {},
   "source": [
    "### Feature Extraction\n",
    "\n",
    "Instead of the MFCC features we used before, let's use the mel-spectrogram as \"raw\" input data.\n",
    "\n",
    "Here's a useful function to compute a mel-spectrogram from a given audio file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea378c2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_mel_spec_for_audio_file(fn_wav,\n",
    "                                    n_fft=1024,  \n",
    "                                    hop_length=441,\n",
    "                                    fs=22050.,\n",
    "                                    n_mels=64):\n",
    "    \"\"\" Compute mel spectrogram\n",
    "    Args:\n",
    "        fn_wav (str): Audio file name\n",
    "        n_fft (int): FFT size\n",
    "        hop_length (int): Hop size in samples\n",
    "        fs (float): Sample rate in Hz\n",
    "        n_mels (int): Number of mel bands\n",
    "    \"\"\"\n",
    "    # load audio samples\n",
    "    x, fs = librosa.load(fn_wav, sr=fs, mono=True)\n",
    "\n",
    "    # normalize to the audio file to an maximum absolute value of 1\n",
    "    if np.max(np.abs(x)) > 0:\n",
    "        x = x / np.max(np.abs(x))\n",
    "\n",
    "    # mel-spectrogram\n",
    "    X = librosa.feature.melspectrogram(y=x,\n",
    "                                       sr=fs,\n",
    "                                       n_fft=n_fft,\n",
    "                                       hop_length=hop_length,\n",
    "                                       n_mels=n_mels,\n",
    "                                       fmin=0.0,\n",
    "                                       fmax=fs / 2,\n",
    "                                       power=1.0,\n",
    "                                       htk=True,\n",
    "                                       norm=None)\n",
    "\n",
    "    # apply dB normalization\n",
    "    X = librosa.amplitude_to_db(X)\n",
    "\n",
    "    return X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdd11d44",
   "metadata": {},
   "source": [
    "Here's how this looks for the first file in our dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcf1097a",
   "metadata": {},
   "outputs": [],
   "source": [
    "mel_spec = compute_mel_spec_for_audio_file(fn_wav_list[0])\n",
    "print(\"Shape of Mel-spectrogram\", mel_spec.shape)\n",
    "\n",
    "pl.figure()\n",
    "pl.imshow(mel_spec, origin=\"lower\", aspect=\"auto\", interpolation=\"None\")\n",
    "pl.ylabel('Mel frequency bands')\n",
    "pl.xlabel('Time frames')\n",
    "pl.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0494ee8",
   "metadata": {},
   "source": [
    "Let's do this over all files and compute all mel spectrograms (*this takes a couple of seconds to compute*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c575b98",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_mel_specs = []\n",
    "for i, fn_wav in enumerate(fn_wav_list):\n",
    "    all_mel_specs.append(compute_mel_spec_for_audio_file(fn_wav_list[i]))\n",
    "    \n",
    "print(\"We have {} spectrograms of shape {}\".format(len(all_mel_specs), all_mel_specs[0].shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "928deb28",
   "metadata": {},
   "source": [
    "The final shape of each spectrogram is ```(64, 251)``` and covers 64 mel frequency bands and 251 time frames.\n",
    "\n",
    "Now we want to do **three things**\n",
    "1. Transpose each spectrogram (switch columns and rows) using the ```.T``` numpy function. The reason is that we want to use each time frame (column) in the spectrogram as individual feature vector. Threfore, we get multiple feature vectors from each file.\n",
    "2. Collect all features vectors from all files. For this, we combine all transposed matrices to one large feature matrix by stacking them vertically below each other.\n",
    "3. Create three vectors, which store for each feature vector the \n",
    "    1. file ID of the orignal file\n",
    "    2. the class number of the original file\n",
    "    3. the within-class number of the original file (e.g. 0 for the first example of its class, 1 for the second etc.)\n",
    "\n",
    "#### Step 1: Transpose the spectrograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd7cdd17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# step 1: transpose all spectrograms (flip rows and columns) and store them in a list\n",
    "all_mel_specs = [spec.T for spec in all_mel_specs]\n",
    "# let's check the new size:\n",
    "print(all_mel_specs[0].shape)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6ff5967",
   "metadata": {},
   "source": [
    "Great, now the energies at the 64 mel frequencies are our features and appear in the second dimension!\n",
    "\n",
    "#### Step 2: Combine all feature matrices (transposed spectrograms) to one large feature matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03aafa81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# step 2:\n",
    "feature_matrix = np.vstack(all_mel_specs)\n",
    "print(\"Feature matrix shape: {}\".format(feature_matrix.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75e327c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "pl.figure(figsize=(3, 10))\n",
    "pl.imshow(feature_matrix, aspect=\"auto\", interpolation=\"None\")\n",
    "pl.ylabel('Time frame')\n",
    "pl.xlabel('Mel frequency band')\n",
    "pl.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25c596e5",
   "metadata": {},
   "source": [
    "**Observation**: Imaging this as a nice visual summary of our small dataset, we see all spectrogram examples for all files, sorted by classes from top to bottom.\n",
    "\n",
    "**Note**: Due to the transposition step, you have to see it as a flipped spectrogram and now the x-axis carries the frequency information and the y-axis is the time axis.\n",
    "\n",
    "#### Step 3: Create a vectors with the file-IDs for each row in the feature matrix and the class-IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "868df3a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_files = len(all_mel_specs)\n",
    "\n",
    "all_file_id = []\n",
    "all_class_id = []\n",
    "all_file_num_in_class = []\n",
    "\n",
    "for cur_file_id in range(n_files):\n",
    "    \n",
    "    # how many time frames does this example have?\n",
    "    cur_n_frames = all_mel_specs[cur_file_id].shape[0]\n",
    "    \n",
    "    # create a vector with file_ids (all the same for all frames of the current spectrogram)\n",
    "    cur_file_id_vec = np.ones(cur_n_frames) * cur_file_id \n",
    "    \n",
    "    # we'll do the same with the class ID associated with the current file\n",
    "    cur_class_id = np.ones(cur_n_frames) * class_id[cur_file_id]\n",
    "    \n",
    "    # and again for the index of the file within each class (0 for the first example per class, 1 for the second ...)\n",
    "    cur_file_num_in_class = np.ones(cur_n_frames) * file_num_in_class[cur_file_id]\n",
    "    \n",
    "    all_file_id.append(cur_file_id_vec)\n",
    "    all_class_id.append(cur_class_id)\n",
    "    all_file_num_in_class.append(cur_file_num_in_class)\n",
    "        \n",
    "# finally, let's concatenate them to two large arrayss\n",
    "all_file_id = np.concatenate(all_file_id)\n",
    "all_class_id = np.concatenate(all_class_id)\n",
    "all_file_num_in_class = np.concatenate(all_file_num_in_class)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7ebd31c",
   "metadata": {},
   "source": [
    "Let's visualize this for better understanding ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db4c8257",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Length of our file_id vector\", len(all_file_id))\n",
    "\n",
    "pl.figure(figsize=(8,4))\n",
    "pl.subplot(2,1,1)\n",
    "pl.plot(all_file_id, label='File ID')\n",
    "pl.plot(all_file_num_in_class, label='File number per class')\n",
    "pl.title('File ID per feature vector')\n",
    "pl.legend()\n",
    "pl.xlabel('')\n",
    "pl.xlim(0, len(all_file_id)-1)\n",
    "pl.subplot(2,1,2)\n",
    "pl.title('Class ID per feature vector')\n",
    "pl.plot(all_class_id)\n",
    "pl.xlim(0, len(all_class_id)-1)\n",
    "pl.tight_layout()\n",
    "pl.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71905690",
   "metadata": {},
   "source": [
    "**Obervation**: \n",
    "\n",
    "*Upper plot*: We can see that we have examples for every of the 25 audio files. The file number per class counts from 0 to 4 (first to 5th example per class) and the starts over again for each class.\n",
    "\n",
    "*Bottom plot*: We can see that the class ID increases from 0 (first class) to 5 (last class)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c44ba4cd",
   "metadata": {},
   "source": [
    "### Dataset split into training and test set\n",
    "\n",
    "We'll start with the code from the previous notebook. Again, we take the first three files per class as training data and the final two files as test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daa349a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "is_train = np.where(all_file_num_in_class <= 2)[0]\n",
    "is_test = np.where(all_file_num_in_class >= 3)[0]\n",
    "\n",
    "print(\"Our feature matrix is split into {} training examples and {} test examples\".format(len(is_train), len(is_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67d3c49b",
   "metadata": {},
   "source": [
    "Now that we have splitted our dataset, we can generate the feature matrix and target vectors for the training and test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccfe450a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = feature_matrix[is_train, :]\n",
    "y_train = all_class_id[is_train]\n",
    "X_test = feature_matrix[is_test, :]\n",
    "y_test = all_class_id[is_test]\n",
    "\n",
    "print(\"Let's look at the dimensions\")\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3361b62",
   "metadata": {},
   "source": [
    "Let's normalize our feature matrix first, this makes sure that all feature dimensions (columns in the feature matrix) have a mean of 0 and a standard deviation of 1. This makes it easier for the classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04d70154",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler().fit(X_train)\n",
    "X_train_norm = scaler.transform(X_train)\n",
    "\n",
    "# apply the scaler also to the test set\n",
    "X_test_norm = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fa43e24",
   "metadata": {},
   "source": [
    "### One-hot encoding\n",
    "\n",
    "Since we want to train a deep neural network, we need to encode the class labels (remember: 0 for the first class, 1 for the second class, 2 for the third class etc.) as binary vectors.\n",
    "\n",
    "The idea is to create for each class label a vector of zeros with only one \"1\" value indicating the annotated class.\n",
    "\n",
    "Example (for 5 classes:\n",
    "\n",
    "```\n",
    "class_id = 0\n",
    "one_hot_vector = [1, 0, 0, 0, 0]\n",
    "\n",
    "class_id = 2\n",
    "one_hot_vector = [0, 0, 1, 0, 0]\n",
    "```\n",
    "\n",
    "Since we have multiple class IDs in ```all_class_ids```, we can use a useful function from *scikit-learn*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1e1e7e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "y_train_transformed = OneHotEncoder(categories='auto', sparse=False).fit_transform(y_train.reshape(-1, 1))\n",
    "y_test_transformed = OneHotEncoder(categories='auto', sparse=False).fit_transform(y_test.reshape(-1, 1))\n",
    "\n",
    "print(\"First four entries as class IDs:\\n{}\".format(y_train[:4]))\n",
    "print(\"First four entries as one-hot-encoded vector:\\n{}\".format(y_train_transformed[:4, :]))\n",
    "\n",
    "print(\"Shape of one-hot-encoded training targets: {}\".format(y_train_transformed.shape))\n",
    "print(\"Shape of one-hot-encoded test targets: {}\".format(y_test_transformed.shape))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47d287b8",
   "metadata": {},
   "source": [
    "### Fully-Connected Neural Network\n",
    "\n",
    "#### Create Model\n",
    "\n",
    "Let's build our neural network. We need the ```Dense``` layers, these are the fully-connected layers, we'll concatenate 3 of them as hidden layers and one as final output layer, which outputs the probabilities for different classes.\n",
    "\n",
    "Between the ```Dense``` layers, we add ```Dropout``` layers. They are \"regularizing\" the model, that means that they randomly set some of the activations inside the network to zero during the model training. This makes the learning task more difficult for the network, at the same time the network learns a more robust model which better generalizes to unseen test data. You can try later to train the model without the dropout layers and compare the performance.\n",
    "\n",
    "We furthermore use the ```Sequential``` model, which allows to easily add more layers to a network.\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d912f6fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Input, Dense, Dropout\n",
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "# let's recall the dimension of our features:\n",
    "n_input_dims = X_train_norm.shape[1]\n",
    "print(\"Our input features have {} dimensions.\".format(n_input_dims))\n",
    "\n",
    "# ... and the number of classes:\n",
    "n_classes = y_train_transformed.shape[1]\n",
    "print(\"We want to classify between {} classes.\".format(n_classes))\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(128, input_dim=n_input_dims, activation='relu'))\n",
    "model.add(Dropout(.2))\n",
    "model.add(Dense(64, activation='relu')) \n",
    "model.add(Dropout(.2))\n",
    "model.add(Dense(64, activation='relu')) \n",
    "\n",
    "# let's add the final layer (output layer)\n",
    "model.add(Dense(n_classes, activation=\"softmax\"))\n",
    " \n",
    "#model.compile(loss='mean_squared_error', optimizer='softmax', metrics=['accuracy'])\n",
    "model.compile(loss='categorical_crossentropy', \n",
    "              optimizer='adam', \n",
    "              metrics=['accuracy'])\n",
    " \n",
    "# Let's have a look on our model:\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c40f71e",
   "metadata": {},
   "source": [
    "#### Train model\n",
    "\n",
    "Now we train the model over $40$ iterations using our training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de3b4ee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(X_train_norm, y_train_transformed, epochs=40, batch_size=32, verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33c8a6f0",
   "metadata": {},
   "source": [
    "**Observations**:\n",
    "\n",
    "- We see that the **loss** value is decreasing and at the same time the **accuracy** is increasing, which show us that the model learns to classify the **training data** better and better over the first 40 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6c4b1fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "pl.figure(figsize=(8, 4))\n",
    "pl.subplot(2,1,1)\n",
    "pl.plot(history.history['loss'])\n",
    "pl.ylabel('Loss')\n",
    "pl.subplot(2,1,2)\n",
    "pl.plot(history.history['accuracy'])\n",
    "pl.ylabel('Accuracy (Training Set)')\n",
    "pl.xlabel('Epoch')\n",
    "pl.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05332658",
   "metadata": {},
   "source": [
    "#### Evaluate model\n",
    "\n",
    "Now we evaluate the model using the **test data** and compute the **accuracy**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f59fe18",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_pred = model.predict(X_test_norm)\n",
    "\n",
    "print(\"Shape of the predictions: {}\".format(y_test_pred.shape))\n",
    "\n",
    "# The model outputs in each row 5 probability values (they always add to 1!) for each class. \n",
    "# We want take the class with the highest probability as prediction!\n",
    "\n",
    "y_test_pred = np.argmax(y_test_pred, axis=1)\n",
    "print(\"Shape of the predictions now: {}\".format(y_test_pred.shape))\n",
    "\n",
    "# Accuracy\n",
    "accuracy = accuracy_score(y_test, y_test_pred)\n",
    "print(\"Accuracy score = \", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dc9384b",
   "metadata": {},
   "source": [
    "For the **evaluation**, we will compute the **accuracy** and the **confusion matrix**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cabfc626",
   "metadata": {},
   "outputs": [],
   "source": [
    "pl.figure(figsize=(3,3))\n",
    "cm = confusion_matrix(y_test, y_test_pred).astype(np.float32)\n",
    "# normalize to probabilities\n",
    "for i in range(cm.shape[0]):\n",
    "    if np.sum(cm[i, :]) > 0:\n",
    "        cm[i, :] /= np.sum(cm[i, :])  # by dividing through the sum, we convert counts to probabilities\n",
    "pl.imshow(cm)\n",
    "ticks = np.arange(5)\n",
    "pl.xticks(ticks, unique_classes)\n",
    "pl.yticks(ticks, unique_classes)\n",
    "pl.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce4e014b",
   "metadata": {},
   "source": [
    "**Observation**\n",
    "\n",
    "This model reaches a very high accuracy on the training set (0.97) and a medium accuracy (around 0.6) on the test set. \n",
    "\n",
    "**Example: Let's try to understand our model better**\n",
    "\n",
    "We'll take the last file as an example, it is part of the test set and assigned to the class \"insects\". Let's visualize the class probabilities frame-by-frame below the actual spectrogram..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49b4bb13",
   "metadata": {},
   "outputs": [],
   "source": [
    "x, fs = librosa.load(fn_wav_list[-1])\n",
    "ipd.display(ipd.Audio(data=x, rate=fs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f1aa697",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract the features\n",
    "mel_spec = compute_mel_spec_for_audio_file(fn_wav_list[-1])\n",
    "\n",
    "# transpose it to a feature matrix\n",
    "feat_mat = mel_spec.T\n",
    "\n",
    "# normalize the features\n",
    "feat_mat = scaler.transform(feat_mat)\n",
    "\n",
    "# compute model predictions\n",
    "class_probs = model.predict(feat_mat)\n",
    "\n",
    "# let's visualize the spectrogram and the predictions\n",
    "pl.figure(figsize=(12, 5))\n",
    "pl.subplot(2,1,1)\n",
    "pl.imshow(mel_spec, origin=\"lower\", aspect=\"auto\", interpolation=\"None\")\n",
    "pl.subplot(2,1,2)\n",
    "pl.imshow(class_probs.T, origin=\"lower\", aspect=\"auto\", interpolation=\"None\")\n",
    "pl.yticks(ticks, unique_classes)\n",
    "pl.tight_layout()\n",
    "pl.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "496533f6",
   "metadata": {},
   "source": [
    "**How to further improve this algorithm?**\n",
    "  - for now, we computed the predictions **per frame**, we can easily aggregate them **to file-level classifications** by **averaging over the frame-level probabilies**\n",
    "  - you can use the MFCC features instead of the mel spectrogram\n",
    "  - try to increase the number of units in the dense layers (first parameter of ```Dense(...)```) or even add more hidden layers\n",
    "  - try to train the model for a longer time (more epochs)\n",
    "  - try another optimizer (see https://analyticsindiamag.com/guide-to-tensorflow-keras-optimizers/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dbc6c38",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b3e28d1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
