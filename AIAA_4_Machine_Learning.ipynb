{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4218b705",
   "metadata": {},
   "source": [
    "# AI-based Audio Analysis of Music and Soundscapes\n",
    "\n",
    "## Machine Learning "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e690b683",
   "metadata": {},
   "source": [
    "## <font color='red'>Programming Session</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "562c119f",
   "metadata": {},
   "source": [
    "This notebook provides concrete examples for machine learning techniques such as unsupervised learning (clustering) and supervised learning (classification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1612f2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sklearn as skl\n",
    "import os\n",
    "import matplotlib\n",
    "import librosa\n",
    "import librosa.display\n",
    "import matplotlib.pyplot as pl\n",
    "import platform\n",
    "import IPython.display as ipd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "889d426f",
   "metadata": {},
   "source": [
    "### Scikit Learn - Basics\n",
    "\n",
    "The website https://scikit-learn.org/stable/ provides a very detailed documentation of the **sklearn** Python library as well as many great tutorials."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23bff157",
   "metadata": {},
   "source": [
    "### Unsupervised Learning - Clustering\n",
    "\n",
    "Scikit-learn provides a lot of different clustering algorithms: https://scikit-learn.org/stable/modules/clustering.html#clustering\n",
    "\n",
    "For simplicity, we will use the $k$-means clustering algorithm (https://scikit-learn.org/stable/modules/clustering.html#k-means) and apply it on a small toy dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b21dc71",
   "metadata": {},
   "source": [
    "#### Dataset\n",
    "\n",
    "Let's define a **feature matrix** ```X``` with two feature dimensions (this could be attributes of images for example) and $9$ data points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c711b1e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(((1,1),\n",
    "              (1, 1.5),\n",
    "              (1.5, 1),\n",
    "              (1, 3),\n",
    "              (1, 4),\n",
    "              (1.5, 4),\n",
    "              (3, 1),\n",
    "              (4, 1),\n",
    "              (4, 1.5)))\n",
    "\n",
    "# let's look at the shape of X\n",
    "n_items, n_dims = X.shape\n",
    "print(\"We have {} data points and {} feature dimensions\".format(n_items, n_dims))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e74749e",
   "metadata": {},
   "source": [
    "Let's visualize our data points..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e812fd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "pl.figure(figsize=(3, 2))\n",
    "pl.plot(X[:, 0], X[:, 1], 'bo')  # first argument: X[:, 0] represents a vector with all \n",
    "                                 # values of the first feature dimension\n",
    "                                 # second argument: X[:, 1] ... second feature dimension\n",
    "                                 # 'bo' includes 'b' for blue and 'o' for circle-shaped markers\n",
    "pl.xlabel('Feature dimension 1')\n",
    "pl.ylabel('Feature dimension 2')\n",
    "pl.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1212776",
   "metadata": {},
   "source": [
    "We first need an instance of the **KMeans** class. \n",
    "\n",
    "Here, we need to define the number of clusters that we expect.\n",
    "Remember, a **cluster** is a group of close data points.\n",
    "If we look to the image above, we see 3 clusters of 3 points each, so we set ```n_clusters=3```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b7ed076",
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans_clustering = skl.cluster.KMeans(n_clusters=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a69b4a16",
   "metadata": {},
   "source": [
    "Now we \"fit\" the clustering algorithm to the data such that it can find the most likely clusters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4130d528",
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans_clustering.fit(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3c84146",
   "metadata": {},
   "source": [
    "Let's look at the algorithm's **predictions**, which are the expected **cluster numbers** each data point is assigned to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5888ec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_numbers = kmeans_clustering.predict(X)\n",
    "print(cluster_numbers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0857590f",
   "metadata": {},
   "source": [
    "We can see that the algorith assigned the first three datapoints to a cluster with the number $2$, the second three to cluster $1$, and the final three points to cluster $0$. The order of the class numbers are not important here. What's important is, which data points end up in the same cluster.\n",
    "\n",
    "Let's visualize this again, this time, we use unique marker shapes and colors for data points in the same cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "597eadf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "pl.figure(figsize=(3, 2))\n",
    "marker_colors = ['r', 'b', 'g']  # red, blue, and green markers\n",
    "marker_shape = ['o', 's', 'd']  # circle, square, diamond shaped markers\n",
    "\n",
    "# now we iterate over all data points and plot them one-by-one\n",
    "for i in range(n_items):\n",
    "    # which cluster number is this data point assigned to?\n",
    "    current_cluster_num = cluster_numbers[i]\n",
    "    pl.plot(X[i, 0], X[i, 1], \n",
    "            marker=marker_shape[current_cluster_num], \n",
    "            color=marker_colors[current_cluster_num])\n",
    "pl.xlabel('Feature dimension 1')\n",
    "pl.ylabel('Feature dimension 2')\n",
    "pl.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "289b51a3",
   "metadata": {},
   "source": [
    "Nice :) This result fits our initial observation, that we as humans can recognize three clusters in the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67ad695e",
   "metadata": {},
   "source": [
    "### Supervised Learning - Classification\n",
    "\n",
    "Now, we want to use the $k$-Nearest-Neighbor classification algorithm discussed in the lecture.\n",
    "We start with the dataset from before, now with class labels.\n",
    "\n",
    "We first need a **training set** which we will use to train our classification model.\n",
    "The training set consists of a feature matrix ```X_train```, which contains for each data point the feature vector, and a target vector ```y_train```, which contains for each data point the corresponding class index / number.\n",
    "We will re-use our dataset from before, now with labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c926d91",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.array(((1,1),\n",
    "                    (1, 1.5),\n",
    "                    (1.5, 1),\n",
    "                    (1, 3),\n",
    "                    (1, 4),\n",
    "                    (1.5, 4),\n",
    "                    (3, 1),\n",
    "                    (4, 1),\n",
    "                    (4, 1.5)))\n",
    "\n",
    "y_train = np.array((0, 0, 0, 1, 1, 1, 2, 2, 2))\n",
    "\n",
    "# number of training examples\n",
    "n_train = len(y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c0bf1b6",
   "metadata": {},
   "source": [
    "Furthermore, we need a **test set**, which we will use to evaluate our classifier later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "610eaec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = np.array(((1.2, 1.2),\n",
    "                   (1.7, 2.7),\n",
    "                   (2, 2),\n",
    "                   (2, 3),\n",
    "                   (3, 3),\n",
    "                   (3.5, 1.5)))\n",
    "\n",
    "y_test = np.array((0, 1, 0, 1, 2, 2))\n",
    "\n",
    "# number of test examples\n",
    "n_test = len(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d08f409c",
   "metadata": {},
   "source": [
    "Let's plot the training dataset and the test data points without labels first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed2ed39d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pl.figure(figsize=(3, 2))\n",
    "\n",
    "# again, unique marker colors and shapes for our classes\n",
    "marker_colors = ['r', 'b', 'g']  \n",
    "marker_shape = ['o', 's', 'd']  \n",
    "\n",
    "# plot the training dataset\n",
    "for i in range(n_train):\n",
    "    # which cluster number is this data point assigned to?\n",
    "    current_cluster_num = cluster_numbers[i]\n",
    "    pl.plot(X_train[i, 0], X_train[i, 1], \n",
    "            marker=marker_shape[current_cluster_num], \n",
    "            color=marker_colors[current_cluster_num])\n",
    "    \n",
    "# plot the test dataset (but without labels for now)\n",
    "for i in range(n_test):\n",
    "    pl.plot(X_test[i, 0], X_test[i, 1], 'ko', markersize=10)\n",
    "    pl.text(X_test[i, 0]+.05, X_test[i, 1]+.2, '?')\n",
    "\n",
    "pl.title('Training set (red, green, blue) + Test set (black)', fontsize=8)\n",
    "pl.xlabel('Feature dimension 1')\n",
    "pl.ylabel('Feature dimension 2')\n",
    "pl.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb1016ec",
   "metadata": {},
   "source": [
    "Now, let's train our **classifier**:\n",
    "1. At first, we will import the Python class of the **kNN classifier** from scikit learn. Note, that we need to specify the number of nearest neighbors to consider.\n",
    "2. Then, we use the **.fit()** function to train the classifier using the training data\n",
    "3. We call the **.predict()** function of our trained classifier to make predictions on the test data, i.e., estimate the class index that each test item refers to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "908c567d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier \n",
    "\n",
    "classifier = KNeighborsClassifier(n_neighbors=3)\n",
    "classifier.fit(X_train, y_train)\n",
    "y_test_predict = classifier.predict(X_test)\n",
    "\n",
    "print(\"Here are the class predictions for the test set: \", y_test_predict)\n",
    "print(\"Here are the true class indices for the test set: \", y_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74bb45bf",
   "metadata": {},
   "source": [
    "What do we observe? The results are not perfect, the classifier makes one incorrect classification (pre-last item). We will evalute this with 2 approaches:\n",
    "1. Accuracy\n",
    "2. Confusion matrix\n",
    "\n",
    "#### Accuracy\n",
    "\n",
    "Accuracy is the percentage of all test items, which were classified correctly. A perfect classifier will give an accuracy of $A=1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "612bf1ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_test_predict)\n",
    "\n",
    "print(\"Accuracy score = \", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9abc5514",
   "metadata": {},
   "source": [
    "#### Confusion matrix\n",
    "\n",
    "The **confusion matrix** shows in detail, which classes are affected by the classification errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f337e8e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "conf_mat = confusion_matrix(y_test, y_test_predict)\n",
    "\n",
    "print(\"Confusion matrix: \")\n",
    "print(conf_mat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d53a706",
   "metadata": {},
   "source": [
    "We can also use a build-in function in **scikit-learn** to plot it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e51cca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import plot_confusion_matrix\n",
    "\n",
    "fig = pl.figure(figsize=(3,3))\n",
    "plot_confusion_matrix(classifier, X_test, y_test, ax=fig.gca(), colorbar=False)\n",
    "pl.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca621718",
   "metadata": {},
   "source": [
    "The confusion matrix shows the correct labels in the rows and the predicted labels in the columns. Here we can observe, that classes 0 and 1 were classified correcty. Our only classification error was between classes 2 and 1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b858785",
   "metadata": {},
   "source": [
    "## Audio Classification Example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee71160b",
   "metadata": {},
   "source": [
    "Let's use our skills on audio processing and machine learning to implement an **animal classification algorithm** :)\n",
    "The dataset used here is a manual selection of 5 examples for 5 animal classes from the https://github.com/karolpiczak/ESC-50 dataset.\n",
    "\n",
    "As the first step, let's get a list of sound classes (animal types) and for each class, a list of audio files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36751d5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "dir_dataset = 'audio/animal_sounds'\n",
    "sub_directories = glob.glob(os.path.join(dir_dataset, '*'))\n",
    "n_sub = len(sub_directories)\n",
    "# let's collect the files in each subdirectory\n",
    "# the folder name is the class name\n",
    "fn_wav_list = []\n",
    "class_label = []\n",
    "file_num_in_class = []\n",
    "\n",
    "for i in range(n_sub):\n",
    "    current_class_label = os.path.basename(sub_directories[i])\n",
    "    current_fn_wav_list = sorted(glob.glob(os.path.join(sub_directories[i], '*.wav')))\n",
    "    for k, fn_wav in enumerate(current_fn_wav_list):\n",
    "        fn_wav_list.append(fn_wav)\n",
    "        class_label.append(current_class_label)\n",
    "        file_num_in_class.append(k)\n",
    "\n",
    "n_files = len(class_label)\n",
    "print('Here is our list of audio files, sorted by sound classes:')\n",
    "for i in range(n_files):\n",
    "    print(class_label[i], '-', fn_wav_list[i])\n",
    "    \n",
    "# this vector includes a \"counter\" for each file within its class, we use it later ...\n",
    "file_num_in_class = np.array(file_num_in_class)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fb13104",
   "metadata": {},
   "source": [
    "Let's listen to one example per class..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c453876",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(5):\n",
    "    idx = 5*i  # always take the first one per class\n",
    "    x, fs = librosa.load(fn_wav_list[idx])\n",
    "    print(class_label[idx])\n",
    "    ipd.display(ipd.Audio(data=x, rate=fs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aba2b64b",
   "metadata": {},
   "source": [
    "We need to get a class ID for each file (which is a number that represents its class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80d86391",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_classes = sorted(list(set(class_label)))\n",
    "print(\"All unique class labels (sorted alphabetically)\", unique_classes)\n",
    "\n",
    "# now we can iterate over all files and look for the index of its class in this list\n",
    "class_id = np.array([unique_classes.index(_) for _ in class_label])\n",
    "\n",
    "print(\"Class IDs of all files\", class_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e83e076a",
   "metadata": {},
   "source": [
    "Let's check the MFCC feature for the first file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcf1097a",
   "metadata": {},
   "outputs": [],
   "source": [
    "x, fs = librosa.load(fn_wav_list[0])\n",
    "mfcc = librosa.feature.mfcc(y=x, n_mfcc=13)\n",
    "print(\"Shape of MFCC matrix\", mfcc.shape)\n",
    "# let's average it over time to get a global feature vector which measures \n",
    "# the overall timbre of the audio file\n",
    "feature_vector = np.mean(mfcc, axis=1)\n",
    "print(\"Shape of our final feature vector\", feature_vector.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0494ee8",
   "metadata": {},
   "source": [
    "Let's do this over all files and combine all **feature vectors** to a **feature matrix** (*this takes a couple of seconds to compute*)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c575b98",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_matrix = []\n",
    "for i, fn_wav in enumerate(fn_wav_list):\n",
    "    x, fs = librosa.load(fn_wav)\n",
    "    mfcc = librosa.feature.mfcc(y=x, n_mfcc=13)\n",
    "    # store current feature vector\n",
    "    feature_matrix.append(np.mean(mfcc, axis=1))\n",
    "# by now, feature_matrix is just a list of one-dimensional numpy arrays.\n",
    "# let's convert it into a two-dimensional array (feature matrix)\n",
    "feature_matrix = np.vstack(feature_matrix) # vertically stacking - row-by-row\n",
    "print(\"Final shape of our feature matrix\", feature_matrix.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "928deb28",
   "metadata": {},
   "source": [
    "The final shape is ```(25 x 13)``` and covers 25 test items (as we have audio files) with 13 feature dimensions.\n",
    "\n",
    "Now, we need to split the data into a **training set** and a **test set**.\n",
    "\n",
    "We use the ```file_num_in_class``` variable from before, which has a counter for each file within it's class files. We will use the first three files in each class as training set and the last two as test set. We use boolean operations to get two masks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54d0e60e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Remember how it looks like:\", file_num_in_class)  # starts at 0 for the first file in each class, etc..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daa349a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "is_train = np.where(file_num_in_class <= 2)[0]\n",
    "is_test = np.where(file_num_in_class >= 3)[0]\n",
    "\n",
    "print(\"Indices of the training set items:\", is_train)\n",
    "print(\"Indices of the test set items:\", is_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67d3c49b",
   "metadata": {},
   "source": [
    "Now that we have splitted our dataset, we can generate the feature matrix and target vectors for the training and test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccfe450a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = feature_matrix[is_train, :]\n",
    "y_train = class_id[is_train]\n",
    "X_test = feature_matrix[is_test, :]\n",
    "y_test = class_id[is_test]\n",
    "\n",
    "print(\"Let's look at the dimensions\")\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3361b62",
   "metadata": {},
   "source": [
    "Let's normalize our feature matrix first, this makes sure that all feature dimensions (columns in the feature matrix) have a mean of 0 and a standard deviation of 1. This makes it easier for the classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04d70154",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler().fit(X_train)\n",
    "X_train_norm = scaler.transform(X_train)\n",
    "\n",
    "print(\"Let's check the mean and standard deviation values over our training set feature matrix for each feature dimension BEFORE...\")\n",
    "print(np.mean(X_train, axis=0))\n",
    "print(np.std(X_train, axis=0))\n",
    "print(\"and AFTER the normalization ...\")\n",
    "print(np.mean(X_train_norm, axis=0))\n",
    "print(np.std(X_train_norm, axis=0))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea4050d1",
   "metadata": {},
   "source": [
    "Let's train our classifier on the training set and compute predictions on the test set.\n",
    "\n",
    "This time, we use a **Random Forest Classifier**. More infos can be found here:\n",
    "  - https://en.wikipedia.org/wiki/Random_forest\n",
    "  - https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html\n",
    "  \n",
    "Similar to the k-Nearest-Neighbor classifier, it implements a **.fit()** and a **.predict()** method.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3111d8b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "classifier = RandomForestClassifier(n_estimators=30)\n",
    "classifier.fit(X_train_norm, y_train)\n",
    "\n",
    "# apply the normalization learnt from the training set\n",
    "X_test_norm = scaler.transform(X_test)\n",
    "y_test_predict = classifier.predict(X_test_norm)\n",
    "\n",
    "print(\"Here are the class predictions for the test set: \", y_test_predict)\n",
    "print(\"Here are the true class indices for the test set: \", y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dc9384b",
   "metadata": {},
   "source": [
    "For the **evaluation**, we will compute the **accuracy** and the **confusion matrix**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23da23eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = accuracy_score(y_test, y_test_predict)\n",
    "print(\"Accuracy score = \", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cabfc626",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = pl.figure(figsize=(3,3))\n",
    "plot_confusion_matrix(classifier, X_test, y_test, ax=fig.gca(), colorbar=False)\n",
    "ticks = np.arange(5)\n",
    "pl.xticks(ticks, unique_classes)\n",
    "pl.yticks(ticks, unique_classes)\n",
    "pl.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "496533f6",
   "metadata": {},
   "source": [
    "How to further improve this algorithm?\n",
    "  - use more data (check the https://github.com/karolpiczak/ESC-50, which provides 40 examples for each class)\n",
    "  - try a different classifier (e.g. Support Vector Machines https://scikit-learn.org/stable/modules/svm.html)\n",
    "  - try different features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e44f9ad7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
