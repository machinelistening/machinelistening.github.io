{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4218b705",
   "metadata": {},
   "source": [
    "# AI-based Audio Analysis of Music and Soundscapes\n",
    "\n",
    "## Research Projects - Useful Hints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2f5b1554",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\abr\\Miniconda3\\envs\\alfpaka_env\\lib\\site-packages\\requests\\__init__.py:104: RequestsDependencyWarning: urllib3 (1.26.6) or chardet (5.0.0)/charset_normalizer (2.0.4) doesn't match a supported version!\n",
      "  RequestsDependencyWarning)\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import os\n",
    "import librosa\n",
    "import tensorflow\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a60abab5",
   "metadata": {},
   "source": [
    "### Read metadata from CSV files\n",
    "\n",
    "We'll use the CSV file https://github.com/karolpiczak/ESC-50/blob/master/meta/esc50.csv from the **ESC-50** as example to show how to import it using the **pandas** library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "587c709e",
   "metadata": {},
   "outputs": [],
   "source": [
    "fn_csv = 'https://raw.githubusercontent.com/karolpiczak/ESC-50/master/meta/esc50.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "94912c3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            filename  fold  target        category  esc10  src_file take\n",
      "0   1-100032-A-0.wav     1       0             dog   True    100032    A\n",
      "1  1-100038-A-14.wav     1      14  chirping_birds  False    100038    A\n",
      "2  1-100210-A-36.wav     1      36  vacuum_cleaner  False    100210    A\n",
      "3  1-100210-B-36.wav     1      36  vacuum_cleaner  False    100210    B\n",
      "4  1-101296-A-19.wav     1      19    thunderstorm  False    101296    A\n",
      "===\n",
      "We have 2000 files\n",
      "---\n",
      "Filename:  1-100032-A-0.wav\n",
      "Class ID:  0\n",
      "Class label:  dog\n",
      "---\n",
      "Filename:  1-100038-A-14.wav\n",
      "Class ID:  14\n",
      "Class label:  chirping_birds\n",
      "---\n",
      "Filename:  1-100210-A-36.wav\n",
      "Class ID:  36\n",
      "Class label:  vacuum_cleaner\n",
      "---\n",
      "Filename:  1-100210-B-36.wav\n",
      "Class ID:  36\n",
      "Class label:  vacuum_cleaner\n",
      "---\n",
      "Filename:  1-101296-A-19.wav\n",
      "Class ID:  19\n",
      "Class label:  thunderstorm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# by default, pandas reads the first line as column \"headers\", which makes sense here.\n",
    "data_frame = pd.read_csv(fn_csv)\n",
    "\n",
    "# let's look at the first 5 rows of the data frame\n",
    "print(data_frame.head())\n",
    "\n",
    "# you can access data from a data frame as:\n",
    "n_files = data_frame.shape[0]\n",
    "print(\"===\")\n",
    "print(\"We have {} files\".format(n_files))\n",
    "\n",
    "# Let's look at the first 5 files\n",
    "for n in range(5):\n",
    "    print(\"---\")\n",
    "    print(\"Filename: \", data_frame[\"filename\"][n])\n",
    "    print(\"Class ID: \", data_frame[\"target\"][n])\n",
    "    print(\"Class label: \", data_frame[\"category\"][n])\n",
    "    \n",
    "# You can iterate over all files easily, extract features etc..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "080accf1",
   "metadata": {},
   "source": [
    "### Extract segments from longer audio files with unified durations\n",
    "\n",
    "This example shows how to take an arbitrary audio file and cut in into segments of a fixed duration (e.g. 1s). This way, you can use collections of multiple audio files with different durations, and create a dataset for a machine learning model:\n",
    "\n",
    "Audio File 1 (34 s) -> 34 audio segments (à 1s)\n",
    "\n",
    "Audio File 2 (12 s) -> 12 audio segments (à 1s)\n",
    "\n",
    "...\n",
    "\n",
    "In total: 46 audio segments.\n",
    "\n",
    "**Note**: Here, we have **no overlap** between the segments. Using overlapping segments is also possible and can potentially give you more data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a51a54d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duration of the audio file in seconds: 3.8458049886621315\n",
      "We get 3 segments of 1 s durations\n",
      "The segments are 22050 samples long\n",
      "Now we have a matrix of shape (3, 22050) which contains the audio samples for each segment in different rows.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# let's take an example from the ESC-50 dataset\n",
    "fn_wav = 'piano.wav'\n",
    "\n",
    "x, fs = librosa.load(fn_wav)\n",
    "\n",
    "len_s = len(x) / fs\n",
    "\n",
    "print('Duration of the audio file in seconds: {}'.format(len_s))\n",
    "\n",
    "segement_len_s = 1\n",
    "\n",
    "n_seg = int(np.floor(len_s / segement_len_s))\n",
    "\n",
    "print('We get {} segments of {} s durations'.format(n_seg, segement_len_s))\n",
    "\n",
    "segment_len_samples = int(segement_len_s*fs)\n",
    "\n",
    "print(\"The segments are {} samples long\".format(segment_len_samples))\n",
    "\n",
    "# now let's collect the audio samples for each segment\n",
    "segment_x = []\n",
    "for s in range(n_seg):\n",
    "    start_sample_index = segment_len_samples*s\n",
    "    end_sample_index = start_sample_index + segment_len_samples\n",
    "    segment_x.append(x[start_sample_index : end_sample_index])\n",
    "    \n",
    "segment_x = np.array(segment_x)\n",
    "\n",
    "print(\"Now we have a matrix of shape {} which contains the audio samples for each segment in different rows.\".format(segment_x.shape))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17e9276c",
   "metadata": {},
   "source": [
    "### Random dataset partition into training and test set \n",
    "\n",
    "Easy example: Consider we have 100 songs, we will first shuffle them (to random order) and then take the first 80% as training data and the second 20% as test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "4e9fd6fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All songs have a unique ID (number): [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47\n",
      " 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71\n",
      " 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95\n",
      " 96 97 98 99]\n",
      "Now the IDs are randomized: [53 35 78 81 36  7 76 91  3 60 86  8 93 56 32 26 61 82  0 11 37 21 31 67\n",
      " 24 19 42 89 98 79  4  1 49 94 85 28 27 75 74 41 44 71 14 96 20 99  6 83\n",
      " 68 43 65 47 45 63 90 64 92 62 46 16 73 66 12 38 29 40 88 54 69 18 34 48\n",
      " 22 70 15 55 77 57 95 84  5 87 80 10 33 97 13 52 58 30 39 51  2 23  9 50\n",
      " 25 59 17 72]\n",
      "We take 80 files as training data\n",
      "Song IDs for the training set: [53 35 78 81 36  7 76 91  3 60 86  8 93 56 32 26 61 82  0 11 37 21 31 67\n",
      " 24 19 42 89 98 79  4  1 49 94 85 28 27 75 74 41 44 71 14 96 20 99  6 83\n",
      " 68 43 65 47 45 63 90 64 92 62 46 16 73 66 12 38 29 40 88 54 69 18 34 48\n",
      " 22 70 15 55 77 57 95 84]\n",
      "Song IDs for the test set: [ 5 87 80 10 33 97 13 52 58 30 39 51  2 23  9 50 25 59 17 72]\n",
      "Class ID [0 2 2 1 3 4 1 5 2 5 1 3 1 3 4 5 4 1 1 2 5 4 2 4 3 1 1 1 4 1 4 4 1 4 0 2 0\n",
      " 2 4 2 1 4 0 3 4 0 1 4 3 4 2 4 3 0 2 2 0 4 4 1 4 0 5 2 4 2 1 3 4 5 1 3 4 2\n",
      " 3 1 5 2 2 3 2 0 3 1 4 4 1 2 1 3 4 5 1 3 1 3 0 2 0 1]\n",
      "Let's check the final shapes\n",
      "(80, 23)\n",
      "(20, 23)\n",
      "(80,)\n",
      "(20,)\n"
     ]
    }
   ],
   "source": [
    "n_files = 100\n",
    "\n",
    "song_id = np.arange(n_files)\n",
    "print(\"All songs have a unique ID (number):\", song_id)\n",
    "\n",
    "# random shuffle\n",
    "np.random.shuffle(song_id)\n",
    "print(\"Now the IDs are randomized:\", song_id)\n",
    "\n",
    "percentage_training_set = 0.8  # we use 80% as training data\n",
    "n_files_train = int(percentage_training_set*n_files)\n",
    "\n",
    "print(\"We take {} files as training data\".format(n_files_train))\n",
    "\n",
    "song_id_train = song_id[:n_files_train]\n",
    "\n",
    "# and the remaining ones as test:\n",
    "song_id_test = song_id[n_files_train:]\n",
    "\n",
    "print(\"Song IDs for the training set:\", song_id_train)\n",
    "print(\"Song IDs for the test set:\", song_id_test)\n",
    "\n",
    "# Now we can use the IDs to split our feature matrix (assuming we have 23 features for each of the 100 files) \n",
    "# and class_id vectors into training and test set\n",
    "feat_mat = np.random.randn(100, 23)\n",
    "\n",
    "feat_mat_train = feat_mat[song_id_train, :]\n",
    "feat_mat_test = feat_mat[song_id_test, :]\n",
    "\n",
    "# dummy example: 5 classes\n",
    "class_id = np.round(5*np.random.rand(100)).astype(int)\n",
    "print(\"Class ID\", class_id)\n",
    "\n",
    "class_id_train = class_id[song_id_train]\n",
    "class_id_test = class_id[song_id_test]\n",
    "\n",
    "print(\"Let's check the final shapes\")\n",
    "print(feat_mat_train.shape)\n",
    "print(feat_mat_test.shape)\n",
    "print(class_id_train.shape)\n",
    "print(class_id_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecbeb70a",
   "metadata": {},
   "source": [
    "### Alternative audio features for specific tasks\n",
    "\n",
    "#### Tonality-based features \n",
    "\n",
    "- Binary templates for chord recognition: https://www.audiolabs-erlangen.de/resources/MIR/FMP/C5/C5S2_ChordRec_Templates.html#Template-Based-Pattern-Matching\n",
    "- Tonal complexity features (statistics over chroma vectors): https://www.audiolabs-erlangen.de/content/05-fau/assistant/00-weiss/01-publications/CIM14_WeissMueller_TonalComplexity.pdf\n",
    "\n",
    "#### Rhythm-based features\n",
    "\n",
    "- Tempogram representation (Tempo vs. Time)\n",
    "  - https://librosa.org/doc/main/_modules/librosa/feature/rhythm.html\n",
    "  - https://www.audiolabs-erlangen.de/resources/MIR/FMP/C6/C6S2_TempogramFourier.html\n",
    "  \n",
    "- Onset detection\n",
    "  - https://librosa.org/doc/main/onset.html\n",
    "  \n",
    "- Beat and tempo\n",
    "  - https://librosa.org/doc/main/beat.html\n",
    "  \n",
    "#### Timbre based features\n",
    "\n",
    "- MFCC, Spectral Centroid, Spectral Flatness, Spectral Bandwidth, Spectral Contrast, Spectral Rolloff\n",
    "  - https://librosa.org/doc/main/feature.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a7ea4e8",
   "metadata": {},
   "source": [
    "### Research Report Structure\n",
    "\n",
    "| Section | Purpose / Content  |\n",
    "| --- | --- |\n",
    "| **Abstract** | Very compact summary of your research report (what is the topic / research field? |\n",
    "|          | give a brief motivation. which methods were applied / compared? | \n",
    "| | what are the main results and conclusions? |\n",
    "| **Introduction** | - classify the research topic in a superordinate field of research |\n",
    "|                  | - introduce / motivate problem, mention possible application scenarios |\n",
    "|                  | - what makes the problem challenging / hard / interesting to look into? |\n",
    "|                  | - briefly list / summarize the main contributions of the paper |\n",
    "| **Related Work** | - **summarize and cluster** (multiple) related publications (journal articles, conference papers, books) by outlining the **main underlying research approaches** to solve the existing problem (**don't** just go through paper 1, paper 2, etc.) |\n",
    "|                  | - **compare and contrast** (explain how other approaches differ from your approach, which other approaches your work builds upon |\n",
    "| **Proposed Method** | - explain your proposed method in details, think about presenting a flow-chart which summarizes the overall workflow |\n",
    "|   | - (individual steps of your flowchart can guide the choice of subsections) |\n",
    "| **Evaluation**  | - possible first subsection: **Dataset / Annotation** (explain source and content and type of annotations) |\n",
    "|   | - explain **evaluation procedure** (dataset split, evaluation metrics) |\n",
    "|   | - (if you perform multiple experiments, this can guide the choice of subsections |\n",
    "| **Results**  | - summarize the main results (tables, figures) |\n",
    "| **Conclusions**  | - summarize the overall result of your paper (list again the contributions, main findings from the experiments) |\n",
    "|   | - optional: provide an outlook on future work |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "626233c5",
   "metadata": {},
   "source": [
    "### Ressources for find related work\n",
    "\n",
    "- https://scholar.google.de/\n",
    "- https://ieeexplore.ieee.org/Xplore/home.jsp\n",
    "- https://arxiv.org/ (**Attention**: these are \"pre-prints\" and have not been checked by a peer-review process, usually you find the newest ideas here. If you want to cite them, check if these pre-prints have been \"properly\" published elsewhere\"."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
